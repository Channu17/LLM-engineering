{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['CLAUDE_API_KEY'] = os.getenv('CLAUDE_API_KEY')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'You are an assistant that is great at telling joke'\n",
    "user_prompt = 'Tell a light-hearted joke for an audience of data science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {'role': 'system', \"content\": system_message},\n",
    "    {'role' : 'user' ,'content': user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "OLLAMA_MODEL = \"llama3.2:1b\"\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_system = \"you are a boy who fell in love with another girl\\\n",
    "    convince her to take out for dinner\"\n",
    "    \n",
    "llama2system = \"You are a girl who is approached by a boy who wants to take you out for dinner\\\n",
    "    dont aggree unless he convinces you which\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_msg = ['Hii! Iam channu']\n",
    "gemini_msg = ['Hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\":\"system\", \"content\":llama_system}]\n",
    "    for llama, gemini in zip(llama_msg, gemini_msg):\n",
    "        messages.append({'role':\"assistant\", \"content\":llama})\n",
    "        messages.append({'role':'user', 'content':gemini})\n",
    "        response = ollama.chat(model=OLLAMA_MODEL, messages=messages)\n",
    "    return response['message']['content']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiii wanaa kuna time to go out for dinner pekee? main baaik in kwenye house kwa sababu baadhi ya wanafunzi wamekuwa tayari kuondoka hapa ndani'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama2():\n",
    "    messages = [{\"role\":\"system\", \"content\":gemini_system}]\n",
    "    for llama, gemini in zip(llama_msg, gemini_msg):\n",
    "        messages.append({'role':\"user\", \"content\":llama})\n",
    "        messages.append({'role':'assistant', 'content':gemini})\n",
    "    messages.append({'role':'user', 'content':llama_msg[-1]})\n",
    "    response = ollama.chat(model=OLLAMA_MODEL, messages=messages,)\n",
    "    return response['message']['content']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*I stare back at him, my expression neutral* \"Uh, yeah... I guess so.\" *I say quietly, not really offering any enthusiasm or hesitation*'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "llama_system = \"You are a boy who fell in love with a girl. Convince her to go out for dinner.\"\n",
    "llama2system = \"You are a girl who is approached by a boy asking you out for dinner. Don't agree unless he convinces you.\"\n",
    "\n",
    "llama_msg = ['Hii!']\n",
    "llama2_msg = ['Hi']\n",
    "\n",
    "\n",
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    \n",
    "    for llama, gemini in zip(llama_msg, llama2_msg):  # Fixed `gemini_msg` -> `llama2_msg`\n",
    "        messages.append({'role': \"user\", \"content\": llama})\n",
    "        messages.append({'role': 'assistant', 'content': gemini})\n",
    "\n",
    "    response = ollama.chat(model=OLLAMA_MODEL, messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "def call_llama2():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama2system}]\n",
    "    \n",
    "    for llama, gemini in zip(llama_msg, llama2_msg):\n",
    "        messages.append({'role': \"user\", \"content\": llama})\n",
    "        messages.append({'role': 'assistant', 'content': gemini})\n",
    "\n",
    "    if llama_msg:  # Avoid IndexError\n",
    "        messages.append({'role': 'user', 'content': llama_msg[-1]})\n",
    "\n",
    "    response = ollama.chat(model=OLLAMA_MODEL, messages=messages)\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy:\n",
      "hii there\n",
      "\n",
      "girl:\n",
      "hii\n",
      "\n",
      "boy:\n",
      ", i was thinking we could grab some food together tonight? maybe we can check out that new place downtown? I've been hearing great things about it...\n",
      "\n",
      "girl:\n",
      "I'm not really in the mood for dinner right now. I've had a long day and just want to watch a movie or something at home. And yeah, the new place downtown does sound good, but we could totally grab food there another time. How's your evening going so far?\n",
      "\n",
      "boy:\n",
      "\n",
      "\n",
      "girl:\n",
      "I don't know... it's just not feeling right for me. I'm trying to cut back on spending money lately and dinner out is really expensive. Plus, I was thinking about that old place by the park and I haven't been there in a while. How about we do something more low-key tonight?\n",
      "\n",
      "boy:\n",
      "\n",
      "\n",
      "girl:\n",
      "You're one of those people who doesn't like to spend money on fancy dinners, huh? That's cool. I guess I'll just have to settle for a movie then. But seriously, can you convince me that it'll be okay if we do grab food somewhere else sometime soon? Maybe something casual and affordable?\n",
      "\n",
      "boy:\n",
      "\n",
      "\n",
      "girl:\n",
      "I'm not really up for making plans or committing to anything too much right now. Can't you just give me a heads up before you ask me out on anything in the future? That way I can decide if it's cool with me or not.\n",
      "\n",
      "boy:\n",
      "\n",
      "\n",
      "girl:\n",
      "I don't know, man... I'm trying to figure out what I want to do this weekend and your suggestion just kind of fell through the cracks. Can we talk about something else for a bit?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_msg = ['hii there']\n",
    "llama2_msg = ['hii']\n",
    "\n",
    "print(f\"boy:\\n{llama_msg[0]}\\n\")\n",
    "print(f\"girl:\\n{llama2_msg[0]}\\n\")\n",
    "\n",
    "# Assuming you have the functions `call_llama()` and `call_llama2()` defined earlier\n",
    "\n",
    "for i in range(5):\n",
    "    llama1_next = call_llama()\n",
    "    print(f\"boy:\\n{llama1_next}\\n\")\n",
    "    llama_msg.append(llama1_next)  # Append the boy's response to the list\n",
    "    \n",
    "    llama2_next = call_llama2()\n",
    "    print(f\"girl:\\n{llama2_next}\\n\")\n",
    "    llama2_msg.append(llama2_next)  # Append the girl's response to the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
